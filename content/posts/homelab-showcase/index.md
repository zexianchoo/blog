---
title: Homelab!!
date: 2025-09-17
summary: On the quest for the ultimate homelab.
tags: ["Homelab"]
author: "Sean"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: true
description: "🤠Homelabbing as a way of life🤠"
# canonicalURL: "https://canonical.url/to/page"
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
UseHugoToc: true
cover:
    image: "homelab-showcase-cover.png"
    alt: "cover"
    caption: "image generated by gemini" 
---
### **If you are here after clicking on my linkedin or resume links, thank you so much. I really appreciate the time and effort checking out my homelab!❤️❤️**

### Before anything, Source Code: 
https://github.com/zexianchoo/homelab-argo-showcase

NOTE: This is a showcase with quite a few potential security concerns redacted, which is why it will not run out of box.

---

# Homelab Journey

I like to think that I've come pretty far with my homelab journey. It started off with my crappy old laptop running [glance](https://github.com/glanceapp/glance) after watching someone make it with docker. Fast forward through the months, and I have went through quite a few different configurations of my homelab.
1. Docker + port-forward
2. Docker Compose + port-forward
3. Terraform + Cloudflare Tunnels
4. Kubernetes + Cloudflare Tunnels
5. Kubernetes + Cloudflare Tunnels + Tailscale

# Current configuration - Diagram
I purchased [my own NAS](https://www.bee-link.com/products/beelink-me-mini-n150?variant=47599172780274), and a few crucial SSDs, and installed Proxmox VE on it.

![diagram](diagram.png)
I have 2 clusters running on Proxmox in their respect Ubuntu VMs, with a bunch of LXCs for other miscellaneous services. One for private, and one for public services.

![proxmox](proxmox.png)

# Proxmox VE Setup - Wireless?

This was epsecially painful because proxmox assumes that you will have an ethernet connection. I didn't have an ethernet cable at home, and I didn't want to spend money on one, so I had to manually download the debian packages for wpa-supplicant and dhclient and transfer it to the mini-pc manually.

This whole process took a pretty long time figuring out dns and wpa-supplicant as it was my first time using it. Having to install packages was also pretty challenging, but I finally managed to do it! I learnt a lot about fundamental system level network configiring, such as debugging with `ip a` and `ip route`.

## Networking

![interfaces](interfaces.png)

I learnt a lot about and `/etc/network/interfaces`. Ultimately, it was a lot of figuring out how to automatically configure the interface on start up with `wpa-supplicant`, and also to get an ip address using `dhcp` with `dhclient`. The image above shows what i ultimately ended up doing.

After the wpa-supplicant set up, I managed to get a semi-static ip (as long as I don't turn this mini-pc off) and ran the proxmox VE installation.

## Proxmox Wifi VM Set-up

It turns out that Wi-Fi adapters can only be used as Linux bridge interface through workarounds, as most Access Points (APs) will reject frames that have a source address that didn’t authenticate with the AP. [(source)](https://www.reddit.com/r/Proxmox/comments/111mkbb/wifi_only_no_ethernet/), [(source)](https://pve.proxmox.com/wiki/WLAN). 

![alt text](bridging.png)

I ended up following the guide [here](https://forum.proxmox.com/threads/how-to-set-up-proxmox-ve-7-on-a-laptop-workstation-with-wifi-wlan.102395/), which use masquarading and Nftables for NAT(Network address translation), in order for packets to be routed from my pve host node to my containers. With that finally out of the way, I was finally able to start making my VM (without really knowing if I had wifi).
![alt text](bridging2.png)

This wifi bridge will be what connects ur vms to the rest of the internet.

I started off by first making the public k8s cluster. I just got an ubuntu iso and plugged it into proxmox, which made it really easy to save and deploy isos. 

The first challenge was trying to figure out what any of the settings on proxmox meant. Some tips:

- If using wifi, just use DHCP
- Defaults are usually fine
- Install the Qemu Agent, it helps with memory balloning (allegedly) and shared node memory. you will have to install it with `sudo apt install qemu-guest-agent` or equivalent after startup.
    - it also tracks ur local subnet ips

![qemu](qemu.png)

Again, if you chose to deploy your NAS on wifi (you probably shouldn't), use the bridge after creating a network device in pve node, under the networking tab. My bridge is called vmbr0 here.
![vmbro](vmbro.png)

aaaand we're in!

---

# Public k8s cluster

First step was to ping google and check that wifi was working. After that, we got the essentials:
1. k3s (bare metal for less overhead, compared to k3d/kind)
2. k9s
3. tailscale (for secure/easy ssh), will cover in later chapter

I used an app of apps pattern, in this manifest [here](https://github.com/zexianchoo/homelab-argo-showcase/blob/main/homelab-argo-public/homelab-root.yaml)

After adding my ESO creds, git creds, and argo creds in a secret manifest, i then started using helm to apply argocd chart.

From then on, its pretty standard k8s stuff.

### Public Applications:
1. Immich 
2. Dashy 
3. Littlelink
4. Planka 
5. Readarr and rreading-glasses
6. Planka
7. cloudflared

### Core services/operators:
1. ESO
2. cert-manager, cloudflare SSL cluster issuer
3. Traefik

Some of the notable comments I have is that using k8s will be kinda overcomplicating your homelab set up. I think docker should've been fine, but I just enjoyed being able to use gitops to control my deployments, rather than having to use ssh. 

Another reason is that vscode server takes up a lot of memory, which I did not have a lot of. I didn't wanna be debugging solely via terminal.

Immich was a particular pain, the chart is actually broken. I will spend some time contributing to upstream sometime soon.

Cloudflare cluster issuer with cert-manager ensured that all traffic is TLS/SSL encrypted, terminating on traefik. To do this, on ur chart ingresses / your own ingress manifest:

1. Make a service that exposes a clusterIP (aka only accessible in-cluster)
2. Point an ingress to the service. Note the `ingressClassName: traefik`, the operator will notice and automatically create ingress routes.
3. Hostname is also important for the DNS challenge proccess for cert issue, unfortunately it will have to be hardcoded, unless you use some crazy patching / kustomize solution.

[Example with dashy](https://github.com/zexianchoo/homelab-argo-public/blob/main/manifests/dashy/dashy.yaml):

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: dashy-service
spec:
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 8080 
    targetPort: 8080 
  selector:
    app: dashy
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dashy-ingress
  namespace: dashy
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod-cloudflare
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
    traefik.ingress.kubernetes.io/router.tls: "true"
spec:
  ingressClassName: traefik
  tls:
  - hosts:
    - seanchoo.top
    secretName: dashy-tls
  rules:
  - host: seanchoo.top
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: dashy-service
            port:
              number: 8080
```

If you are interested in learning how to set up cloudflare, check out my other post [here](https://blog.seanchoo.top/posts/homelab-intro/).

# Tailscale
Before talking about the private cluster, we will have to talk about tailscale as the VPN. It is a private mesh vpn which uses wireguard tunnels as its backbone. Configuration and authentication is supported by tailscale itself, but otherwise you could realistically make your own.

## Authentication Service / IDP (Identity provider):
I chose to self host my IdP because I didn't want to tie my tailscale account to my social accounts. Also to flex i guess? 

I Chose Authentik because I was familiar with it, although I have heard that Authelia is easier, especially when you aren't using k8s.

I hosted my IdP on a VPS (heztner-cloud) so that I don't have to worry about it being down. Then, you will have to follow the steps listed [here](https://integrations.goauthentik.io/networking/tailscale/).

It wasn't too hard to set up, and I used cloudflare tunels here too. One big big big point I want to make is that if you are hosting your VPS on non-conventional clouds (cough cough heztner cloud), any auth requests to your VPS may be flagged as bot activity. You may have to go over to the cloudflare dashboard > rules and fix this with a custom rule to allow requests with headers containing `tailscale` or similar through the tunnel. 

You may get a 500 auth servive initialization error otherwise. (and the worst part is that it kicks in only after a few hours too, thanks cloudflare)

After getting my IdP set up, tailscale isn't too difficult to set up. The harder part is the tailscale kubernetes operator.


# Private k8s cluster
![private](private.png)
Again, I made a new VM and got the essentials (see above), with essentially the same setup as before.

### Private Applications:
1. Immich
2. Jellyfin
3. Dawarich
4. Paperless-ngx
5. Uptime Kuma
6. gethomepage

### Core Services:
1. Tailscale
2. fail2ban

## Tailscale K8s Operator

[This documentation page](https://tailscale.com/kb/1439/kubernetes-operator-cluster-ingress) shows pretty much how to set most of it up. There are a few tips I have to share:

1. You define host names via tls.
``` yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx
spec:
  defaultBackend:
    service:
      name: nginx
      port:
        number: 80
  ingressClassName: tailscale
  tls:
    - hosts:
        - nginx
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  ports:
    - name: https
      port: 443
      targetPort: 443
  type: ClusterIP
```


notice the part at 
```yaml
tls:
    - hosts:
        - nginx
```

This means that this will correspond to `nginx.<your tailnet domain>.ts.net`. do NOT put the host as the literal `nginx.<your tailnet domain>.ts.net`, it breaks.

2. If you need to restart tailscale operator / restart deployments / "sidecar" pod proxies for any reason, please remember to delete the ts secret, as well as stateful set when tearing down any deployments. The ts secrets will hold stale, yet critical info into the next deployment.
3. On first connection to the exposed url, wait for 1-2 mins. The tailscale operator will have to run a DNS challenge for your ingress cert. You can check the logs of the proxy pod to see this in action.

## CoreDNS Issues with tailscale
it turns out that DNS daemon systemd-resolve will fight with tailscale operator for control over `/etc/resolv.conf`. One way to have connection to the internet, while having in-cluster DNS is to maintain the original `/etc/resolv.conf` (i.e. default nameserver), while setting up a custom CoreDNS configmap(?)

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns-custom
  namespace: kube-system
  labels:
    coredns.io/custom: "true"
data:
  tailscale.server: |
    <tailnet.name>:53 {
        errors
        cache 30
        forward . 100.100.100.100
    }
```
This is pretty barebones, you can do with retries and timeouts, but it does the job for in-cluster networking. This means that we can get stuff like uptime kuma tracking urls instead of finnicky ip addresses.

(dont mind the downed dawarich lol)
![uptime](uptime.png)

And this is my private homepage! Pretty barebones but it is under development. I was inspired by [this post](https://www.reddit.com/r/selfhosted/comments/1lv8acd/2_years_self_hosted_finally_proud): 
![homepage](home.png)

# LXCs (Linux Containers)

Why LXCs? Performance and memory! Do note that with tailscale for ssh or such, you will have to do some shennanigans with the LXC conf in ur main pve node if they are unprivileged (as they should be).

https://tailscale.com/kb/1130/lxc-unprivileged

I set up the LXC for my adguard-home, as well as for my blog that you are reading this on. I think the setup for the LXC containers are mostly trivial after making the VMs.

# Memory issues:

I run out of memory pretty quick (2 immich deployments, k8s overhead etc). Some tips I have for proxmox are as follows:

1. Use qemu guest agent for memory ballooning. you can check the free memory via `free -h`.
2. The machines will only respect memory usage when you set the minimum memory use over here:

![memory](memory.png)

The (linux) OS will greedily eat up spare memory for caching and performance. 
3. Use SWAP memory. To safeguard against OOM, I used swap memory for those instances of high memory use spikes.

```bash
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```
check if it worked via `free -h`

# Conclusion

The countless hours sitting at cafes with cappucinos, the countless late nights I've spent trying to fix a dumb issue, and the countless days of dedication really gave so much back. 

I really poured my heart into this homelab. I cannot stress this enough - college doesn't teach you much of this! Learning how to actually set up servers has taught me so much appreciation for the amazing ecosystem we have today we call the internet.

This also made me a much better programmer. Debugging all of these networking challenges (where you can't really rely on chatGPT because your situation is usually unique and not in code) has been great, practical practice.

my dear mini-pc is probably clinging on for its life, but despite all the abuse it has taken, it is something im so proud of :)

If you read all the way here, thank you so much for being so patient. I appreciate you for taking the time to indulge me in my little spiel haha

till next time!

---

### Links
- dashy homepage: https://seanchoo.top
- github repo: https://github.com/zexianchoo/homelab-argo-showcase

